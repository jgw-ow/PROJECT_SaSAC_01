{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 로드 성공: articles_20대_소비_2020.csv\n",
      "\n",
      "=== articles_20대_소비_2020.csv의 토픽 ===\n",
      "토픽 0: 0.002*\"코로나\" + 0.001*\"상하이지부장은\" + 0.001*\"무역협회\" + 0.001*\"컬러를\" + 0.001*\"덕후\"\n",
      "토픽 1: 0.004*\"코로나\" + 0.001*\"개정\" + 0.001*\"감소했지만\" + 0.001*\"MZ세대\" + 0.001*\"청량하고\"\n",
      "토픽 2: 0.003*\"코로나\" + 0.001*\"스트리밍\" + 0.001*\"원장은\" + 0.001*\"공식\" + 0.001*\"강한\"\n",
      "토픽 3: 0.006*\"코로나\" + 0.002*\"SNS\" + 0.002*\"트위터\" + 0.001*\"감소했다\" + 0.001*\"건이었으며\"\n",
      "토픽 4: 0.004*\"코로나\" + 0.002*\"경험이\" + 0.001*\"원장\" + 0.001*\"게임으로\" + 0.001*\"만든\"\n",
      "파일 로드 성공: articles_20대_소비_2021.csv\n",
      "\n",
      "=== articles_20대_소비_2021.csv의 토픽 ===\n",
      "토픽 0: 0.004*\"코로나\" + 0.001*\"지출은\" + 0.001*\"사회적으로\" + 0.001*\"의료비\" + 0.001*\"역대\"\n",
      "토픽 1: 0.003*\"코로나\" + 0.002*\"MZ세대\" + 0.001*\"MZ세대를\" + 0.001*\"MZ세대의\" + 0.001*\"영역에\"\n",
      "토픽 2: 0.002*\"코로나\" + 0.002*\"MZ세대\" + 0.002*\"히키코모리\" + 0.001*\"관련해\" + 0.001*\"선거와\"\n",
      "토픽 3: 0.002*\"코로나\" + 0.001*\"활동\" + 0.001*\"경험\" + 0.001*\"MZ세대\" + 0.001*\"분야에\"\n",
      "토픽 4: 0.004*\"코로나\" + 0.002*\"경험이\" + 0.001*\"대표적인\" + 0.001*\"MZ세대가\" + 0.001*\"MZ세대\"\n",
      "파일 로드 성공: articles_20대_소비_2022.csv\n",
      "\n",
      "=== articles_20대_소비_2022.csv의 토픽 ===\n",
      "토픽 0: 0.001*\"MZ세대가\" + 0.001*\"염\" + 0.001*\"역사를\" + 0.001*\"것인가를\" + 0.001*\"필두로\"\n",
      "토픽 1: 0.001*\"MZ세대의\" + 0.001*\"역량이\" + 0.001*\"알뜰폰이\" + 0.001*\"만족률로\" + 0.001*\"였으며\"\n",
      "토픽 2: 0.005*\"코로나\" + 0.002*\"분석을\" + 0.001*\"제공\" + 0.001*\"신한카드\" + 0.001*\"경험이\"\n",
      "토픽 3: 0.003*\"코로나\" + 0.002*\"여가활동\" + 0.002*\"활동을\" + 0.002*\"활동\" + 0.001*\"여가생활\"\n",
      "토픽 4: 0.003*\"코로나\" + 0.002*\"MZ세대\" + 0.001*\"Z세대를\" + 0.001*\"공략하기\" + 0.001*\"치러진\"\n",
      "파일 로드 성공: articles_20대_소비_2023.csv\n",
      "\n",
      "=== articles_20대_소비_2023.csv의 토픽 ===\n",
      "토픽 0: 0.002*\"줄어\" + 0.001*\"꺼리는\" + 0.001*\"후반\" + 0.001*\"날씨가\" + 0.001*\"요기요도\"\n",
      "토픽 1: 0.003*\"MZ가\" + 0.001*\"시성비\" + 0.001*\"덜\" + 0.001*\"왔다\" + 0.001*\"보는\"\n",
      "토픽 2: 0.001*\"업계에서는\" + 0.001*\"이다\" + 0.001*\"경제적\" + 0.001*\"조원\" + 0.001*\"MZ세대\"\n",
      "토픽 3: 0.001*\"코로나\" + 0.001*\"유기가공\" + 0.001*\"주택가격지수\" + 0.001*\"불구속\" + 0.001*\"시카고\"\n",
      "토픽 4: 0.001*\"정치에\" + 0.001*\"등록\" + 0.001*\"연령층보다\" + 0.001*\"관세법과\" + 0.001*\"마리당\"\n",
      "파일 로드 성공: articles_20대_소비_2024.csv\n",
      "\n",
      "=== articles_20대_소비_2024.csv의 토픽 ===\n",
      "토픽 0: 0.001*\"안에\" + 0.001*\"비비\" + 0.001*\"이상에서는\" + 0.001*\"선호\" + 0.001*\"들지\"\n",
      "토픽 1: 0.002*\"경험이\" + 0.002*\"SNS\" + 0.001*\"세계라면협회\" + 0.001*\"저렴해서\" + 0.001*\"WINA\"\n",
      "토픽 2: 0.001*\"방송을\" + 0.001*\"제의\" + 0.001*\"국가사업으로\" + 0.001*\"최우수\" + 0.001*\"최우수기관\"\n",
      "토픽 3: 0.009*\"경험은\" + 0.001*\"PT\" + 0.001*\"적은\" + 0.001*\"한다는\" + 0.001*\"불린다\"\n",
      "토픽 4: 0.003*\"경험이\" + 0.001*\"점은\" + 0.001*\"쇼핑채널로\" + 0.001*\"내는\" + 0.001*\"문제를\"\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 lda 분석\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_path = r\"C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\"\n",
    "stopwords_path = os.path.join(base_path, \"stopwords_news.txt\")\n",
    "\n",
    "# 불용어 파일 읽기 함수\n",
    "def load_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            stop_words = set(line.strip() for line in f)\n",
    "        return stop_words\n",
    "    except FileNotFoundError:\n",
    "        print(f\"불용어 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return set()\n",
    "\n",
    "# 텍스트 전처리 함수 (간단한 정규식 기반 토크나이저 사용)\n",
    "def preprocess_text(text, stopwords_path):\n",
    "    stop_words = load_stopwords(stopwords_path)  # 최신 불용어 목록 로드\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # 단어 기준 토크나이저\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # 대괄호 내용 제거\n",
    "    text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "    tokens = tokenizer.tokenize(text)  # 텍스트를 단어 단위로 토크나이징\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # 불용어 제거\n",
    "    return tokens\n",
    "\n",
    "# LDA 분석 함수\n",
    "def lda_analysis(file_name, stopwords_path, num_topics=5, passes=10):\n",
    "    try:\n",
    "        # 파일 경로 설정\n",
    "        file_path = os.path.join(base_path, file_name)\n",
    "        \n",
    "        # 데이터 로드\n",
    "        data = pd.read_csv(file_path, encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "        print(f\"파일 로드 성공: {file_name}\")\n",
    "\n",
    "        # 데이터 컬럼 이름을 소문자로 변환\n",
    "        data.columns = data.columns.str.lower()\n",
    "\n",
    "        # 'description' 컬럼 전처리\n",
    "        data['description'] = data['description'].fillna('')  # 결측값 처리\n",
    "        data['processed'] = data['description'].apply(lambda text: preprocess_text(text, stopwords_path))\n",
    "\n",
    "        # 말뭉치 및 사전 생성\n",
    "        dictionary = corpora.Dictionary(data['processed'])\n",
    "        corpus = [dictionary.doc2bow(text) for text in data['processed']]\n",
    "\n",
    "        # LDA 모델 학습\n",
    "        lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "\n",
    "        # 토픽 출력\n",
    "        print(f\"\\n=== {file_name}의 토픽 ===\")\n",
    "        topics = lda_model.print_topics(num_words=5)\n",
    "        for idx, topic in topics:\n",
    "            print(f\"토픽 {idx}: {topic}\")\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"파일을 찾을 수 없습니다: {fnf_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{file_name} 처리 중 오류 발생: {e}\")\n",
    "\n",
    "# 5개년 데이터 처리\n",
    "file_names = [f\"articles_20대_소비_{year}.csv\" for year in range(2020, 2025)]\n",
    "for file_name in file_names:\n",
    "    lda_analysis(file_name, stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from wordcloud) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from wordcloud) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl (301 kB)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 데이터 로드 시작 ===\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2020.csv\n",
      "articles_20대_소비_2020.csv 데이터를 그룹1에 추가했습니다.\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2021.csv\n",
      "articles_20대_소비_2021.csv 데이터를 그룹1에 추가했습니다.\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2022.csv\n",
      "articles_20대_소비_2022.csv 데이터를 그룹2에 추가했습니다.\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2023.csv\n",
      "articles_20대_소비_2023.csv 데이터를 그룹2에 추가했습니다.\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2024.csv\n",
      "articles_20대_소비_2024.csv 데이터를 그룹2에 추가했습니다.\n",
      "\n",
      "=== 데이터 로드 완료 ===\n",
      "그룹1 (2020-2021년) 데이터 수: 1400\n",
      "그룹2 (2022-2024년) 데이터 수: 2128\n",
      "2020-2021년 워드 클라우드 저장 완료: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\2020-2021년.png\n",
      "2022-2024년 워드 클라우드 저장 완료: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\2022-2024년.png\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 wordcloud\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import chardet\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_path = r\"C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\"\n",
    "stopwords_path = os.path.join(base_path, \"stopwords_news.txt\")\n",
    "\n",
    "# 한글 폰트 설정\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"  # Windows의 경우\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "# 불용어 파일 읽기 함수\n",
    "def load_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            stop_words = set(line.strip() for line in f)\n",
    "        return stop_words\n",
    "    except FileNotFoundError:\n",
    "        print(f\"불용어 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return set()\n",
    "\n",
    "# 텍스트 전처리 함수 (간단한 정규식 기반 토크나이저 사용)\n",
    "def preprocess_text(text, stopwords_path):\n",
    "    stop_words = load_stopwords(stopwords_path)  # 최신 불용어 목록 로드\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # 단어 기준 토크나이저\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # 대괄호 내용 제거\n",
    "    text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "    tokens = tokenizer.tokenize(text)  # 텍스트를 단어 단위로 토크나이징\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # 불용어 제거\n",
    "    return tokens\n",
    "\n",
    "# 워드 클라우드 생성 함수\n",
    "def generate_wordcloud(title, texts):\n",
    "    try:\n",
    "        # 모든 텍스트를 하나로 합침\n",
    "        all_text = ' '.join(texts)\n",
    "        # 전처리\n",
    "        tokens = preprocess_text(all_text, stopwords_path)\n",
    "        # 단어 빈도 계산\n",
    "        freq = defaultdict(int)\n",
    "        for word in tokens:\n",
    "            freq[word] += 1\n",
    "        if not freq:\n",
    "            print(f\"{title} 워드 클라우드에 사용할 단어가 없습니다.\")\n",
    "            return\n",
    "        # 워드 클라우드 생성\n",
    "        wordcloud = WordCloud(\n",
    "            font_path=font_path,  # 한글 폰트 경로 설정\n",
    "            background_color='white',\n",
    "            width=800,\n",
    "            height=600\n",
    "        ).generate_from_frequencies(freq)\n",
    "        # 워드 클라우드 시각화\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.tight_layout(pad=0)\n",
    "        # 워드 클라우드를 이미지 파일로 저장\n",
    "        save_path = os.path.join(base_path, f\"{title}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"{title} 워드 클라우드 저장 완료: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{title} 워드 클라우드 생성 중 오류 발생: {e}\")\n",
    "\n",
    "# 파일 인코딩 감지 함수\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "# 데이터 로드 및 그룹화\n",
    "def load_and_group_data(file_names, group1_years, group2_years):\n",
    "    group1_data = []\n",
    "    group2_data = []\n",
    "    print(\"\\n=== 데이터 로드 시작 ===\")\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(base_path, file_name)\n",
    "        print(f\"\\n파일 로드: {file_path}\")\n",
    "        encoding = detect_encoding(file_path)\n",
    "        data = pd.read_csv(file_path, encoding=encoding, on_bad_lines='skip', engine='python')\n",
    "        if data is not None:\n",
    "            # 데이터 컬럼 이름을 소문자로 변환 (문자열만)\n",
    "            data.columns = [col.lower() if isinstance(col, str) else col for col in data.columns]\n",
    "            # 텍스트 컬럼 자동 탐색\n",
    "            text_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "            if not text_columns:\n",
    "                print(f\"문자열 데이터가 있는 컬럼이 {file_name}에 존재하지 않습니다.\")\n",
    "                continue\n",
    "            # 우선 'description' 컬럼이 있으면 사용, 없으면 첫 번째 문자열 컬럼 사용\n",
    "            if 'description' in text_columns:\n",
    "                text_col = 'description'\n",
    "            else:\n",
    "                text_col = text_columns[0]\n",
    "                print(f\"'description' 컬럼이 {file_name}에 존재하지 않습니다. 대신 '{text_col}' 컬럼을 사용합니다.\")\n",
    "            # 선택된 컬럼의 데이터를 문자열로 변환\n",
    "            data[text_col] = data[text_col].apply(lambda x: str(x) if not isinstance(x, str) else x).fillna('')\n",
    "            # 문서 추출\n",
    "            documents = data[text_col].tolist()\n",
    "            if not documents:\n",
    "                print(f\"{file_name}에서 문서를 추출하지 못했습니다.\")\n",
    "                continue\n",
    "            # 그룹에 따라 데이터 추가\n",
    "            year_num = re.findall(r'\\d{4}', file_name)[0]\n",
    "            if year_num in group1_years:\n",
    "                group1_data.extend(documents)\n",
    "                print(f\"{file_name} 데이터를 그룹1에 추가했습니다.\")\n",
    "            elif year_num in group2_years:\n",
    "                group2_data.extend(documents)\n",
    "                print(f\"{file_name} 데이터를 그룹2에 추가했습니다.\")\n",
    "            else:\n",
    "                print(f\"{file_name}은 정의된 그룹에 속하지 않습니다.\")\n",
    "        else:\n",
    "            print(f\"{file_name} 데이터 로드에 실패했습니다.\")\n",
    "    # 데이터 확인\n",
    "    print(\"\\n=== 데이터 로드 완료 ===\")\n",
    "    print(f\"그룹1 (2020-2021년) 데이터 수: {len(group1_data)}\")\n",
    "    print(f\"그룹2 (2022-2024년) 데이터 수: {len(group2_data)}\")\n",
    "    return group1_data, group2_data\n",
    "\n",
    "# 5개년 데이터 처리\n",
    "file_names = [f\"articles_20대_소비_{year}.csv\" for year in range(2020, 2025)]\n",
    "group1_years = ['2020', '2021']\n",
    "group2_years = ['2022', '2023', '2024']\n",
    "\n",
    "group1_data, group2_data = load_and_group_data(file_names, group1_years, group2_years)\n",
    "\n",
    "# 워드 클라우드 생성 실행\n",
    "if group1_data:\n",
    "    generate_wordcloud(\"2020-2021년\", group1_data)\n",
    "else:\n",
    "    print(\"그룹1에 데이터가 없습니다.\")\n",
    "if group2_data:\n",
    "    generate_wordcloud(\"2022-2024년\", group2_data)\n",
    "else:\n",
    "    print(\"그룹2에 데이터가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== naver_blog_2020.json의 토픽 ===\n",
      "토픽 0: 0.001*\"직접\" + 0.001*\"보니\" + 0.001*\"사람이\" + 0.001*\"줄\" + 0.001*\"만든\"\n",
      "토픽 1: 0.001*\"매장\" + 0.001*\"직접\" + 0.001*\"만든\" + 0.001*\"성수동에\" + 0.001*\"사람들이\"\n",
      "토픽 2: 0.001*\"입장\" + 0.001*\"성수동에\" + 0.001*\"주년을\" + 0.001*\"침대\" + 0.001*\"창립\"\n",
      "토픽 3: 0.001*\"만든\" + 0.001*\"생각보다\" + 0.001*\"com\" + 0.001*\"직접\" + 0.001*\"사람이\"\n",
      "토픽 4: 0.001*\"오랜만에\" + 0.001*\"한번\" + 0.001*\"생각보다\" + 0.001*\"가서\" + 0.001*\"직접\"\n",
      "\n",
      "=== naver_blog_2021.json의 토픽 ===\n",
      "토픽 0: 0.001*\"생각보다\" + 0.001*\"전시\" + 0.001*\"제일\" + 0.001*\"한번\" + 0.001*\"보이는\"\n",
      "토픽 1: 0.001*\"ㅋ\" + 0.001*\"아케인\" + 0.001*\"만든\" + 0.001*\"않고\" + 0.001*\"층에\"\n",
      "토픽 2: 0.001*\"한번\" + 0.001*\"만든\" + 0.001*\"향수\" + 0.001*\"생각보다\" + 0.001*\"마음에\"\n",
      "토픽 3: 0.001*\"만든\" + 0.001*\"전시\" + 0.001*\"예약\" + 0.001*\"직접\" + 0.001*\"생각보다\"\n",
      "토픽 4: 0.001*\"FACTORY\" + 0.001*\"직접\" + 0.001*\"전시\" + 0.001*\"한번\" + 0.001*\"만나볼\"\n",
      "\n",
      "=== naver_blog_2022.json의 토픽 ===\n",
      "토픽 0: 0.001*\"직접\" + 0.001*\"만나볼\" + 0.001*\"콜라보\" + 0.001*\"저도\" + 0.001*\"이벤트\"\n",
      "토픽 1: 0.001*\"직접\" + 0.001*\"어딕트\" + 0.001*\"예약\" + 0.001*\"생각보다\" + 0.001*\"한번\"\n",
      "토픽 2: 0.001*\"com\" + 0.001*\"https\" + 0.001*\"느낌\" + 0.001*\"직접\" + 0.001*\"찍고\"\n",
      "토픽 3: 0.002*\"리복\" + 0.002*\"써모스\" + 0.001*\"직접\" + 0.001*\"찍고\" + 0.001*\"파이롯트\"\n",
      "토픽 4: 0.002*\"까멜리아\" + 0.002*\"레드\" + 0.001*\"직접\" + 0.001*\"DE\" + 0.001*\"N\"\n",
      "\n",
      "=== naver_blog_2023.json의 토픽 ===\n",
      "토픽 0: 0.001*\"직접\" + 0.001*\"이벤트\" + 0.001*\"예약\" + 0.001*\"저도\" + 0.001*\"증정\"\n",
      "토픽 1: 0.002*\"오레오\" + 0.002*\"직접\" + 0.001*\"이벤트\" + 0.001*\"만나볼\" + 0.001*\"스파이더맨\"\n",
      "토픽 2: 0.002*\"새로\" + 0.002*\"몰티져스\" + 0.001*\"직접\" + 0.001*\"만나볼\" + 0.001*\"예약\"\n",
      "토픽 3: 0.002*\"라이카\" + 0.001*\"노르웨이\" + 0.001*\"직접\" + 0.001*\"예약\" + 0.001*\"어그\"\n",
      "토픽 4: 0.003*\"콜라\" + 0.002*\"이벤트\" + 0.002*\"엡손\" + 0.002*\"직접\" + 0.001*\"향수\"\n",
      "\n",
      "=== naver_blog_2024.json의 토픽 ===\n",
      "토픽 0: 0.002*\"직접\" + 0.002*\"취\" + 0.002*\"이벤트\" + 0.001*\"만나볼\" + 0.001*\"향수\"\n",
      "토픽 1: 0.002*\"직접\" + 0.001*\"예약\" + 0.001*\"팝업은\" + 0.001*\"레티놀\" + 0.001*\"이벤트\"\n",
      "토픽 2: 0.003*\"용가리\" + 0.002*\"직접\" + 0.002*\"에르노\" + 0.002*\"만나볼\" + 0.002*\"쿠션\"\n",
      "토픽 3: 0.003*\"짐빔\" + 0.002*\"하이볼\" + 0.001*\"직접\" + 0.001*\"피자\" + 0.001*\"싶은\"\n",
      "토픽 4: 0.002*\"이벤트\" + 0.002*\"직접\" + 0.001*\"현장\" + 0.001*\"루나\" + 0.001*\"만나볼\"\n"
     ]
    }
   ],
   "source": [
    "# 블로그 lda 분석\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_path = r\"C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\"\n",
    "stopwords_path = os.path.join(base_path, \"stopwords_blog.txt\")\n",
    "\n",
    "# 불용어 파일 읽기 함수\n",
    "def load_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            stop_words = set(line.strip() for line in f)\n",
    "        # print(f\"불용어 목록 ({len(stop_words)}개): {list(stop_words)[:10]}...\")  # 불용어 목록 일부 출력\n",
    "        return stop_words\n",
    "    except FileNotFoundError:\n",
    "        print(f\"불용어 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return set()\n",
    "\n",
    "# 텍스트 전처리 함수 (간단한 정규식 기반 토크나이저 사용)\n",
    "def preprocess_text(text, stop_words):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # 단어 기준 토크나이저\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # 대괄호 내용 제거\n",
    "    text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "    tokens = tokenizer.tokenize(text)  # 텍스트를 단어 단위로 토크나이징\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # 불용어 제거\n",
    "    # print(f\"전처리된 텍스트: {tokens[:10]}...\")  # 전처리된 텍스트 일부 출력\n",
    "    return tokens\n",
    "\n",
    "# LDA 분석 함수\n",
    "def lda_analysis(file_name, stopwords_path, num_topics=5, passes=10):\n",
    "    try:\n",
    "        # 파일 경로 설정\n",
    "        file_path = os.path.join(base_path, file_name)\n",
    "        \n",
    "        # 데이터 로드\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # 불용어 목록 로드\n",
    "        stop_words = load_stopwords(stopwords_path)\n",
    "        \n",
    "        # 데이터 전처리\n",
    "        all_texts = []\n",
    "        for year_data in data:\n",
    "            for entry in year_data.get(file_name.split('_')[2].split('.')[0], []):  # 연도 추출하여 사용\n",
    "                content = entry.get('content', '')\n",
    "                processed_text = preprocess_text(content, stop_words)\n",
    "                if len(processed_text) > 5:  # 전처리된 텍스트가 일정 길이 이상인 경우에만 추가\n",
    "                    all_texts.append(processed_text)\n",
    "        \n",
    "        if not all_texts:\n",
    "            print(f\"{file_name}에 유효한 텍스트가 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        # 말뭉치 및 사전 생성\n",
    "        dictionary = corpora.Dictionary(all_texts)\n",
    "        corpus = [dictionary.doc2bow(text) for text in all_texts]\n",
    "\n",
    "        # LDA 모델 학습\n",
    "        lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "\n",
    "        # 토픽 출력\n",
    "        print(f\"\\n=== {file_name}의 토픽 ===\")\n",
    "        topics = lda_model.print_topics(num_words=5)\n",
    "        for idx, topic in topics:\n",
    "            print(f\"토픽 {idx}: {topic}\")\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"파일을 찾을 수 없습니다: {fnf_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{file_name} 처리 중 오류 발생: {e}\")\n",
    "\n",
    "# JSON 파일 목록\n",
    "file_names = [\n",
    "    \"naver_blog_2020.json\",\n",
    "    \"naver_blog_2021.json\",\n",
    "    \"naver_blog_2022.json\",\n",
    "    \"naver_blog_2023.json\",\n",
    "    \"naver_blog_2024.json\"\n",
    "]\n",
    "\n",
    "# 각 파일에 대해 LDA 분석 수행\n",
    "for file_name in file_names:\n",
    "    lda_analysis(file_name, stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-2021년 워드 클라우드 저장 완료: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\2020-2021년.png\n",
      "2022-2024년 워드 클라우드 저장 완료: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\2022-2024년.png\n"
     ]
    }
   ],
   "source": [
    "# 블로그 wordcloud\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_path = r\"C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\"\n",
    "stopwords_path = os.path.join(base_path, \"stopwords_blog.txt\")\n",
    "\n",
    "# 한글 폰트 설정\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"  # Windows의 경우\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "# 불용어 파일 읽기 함수\n",
    "def load_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            stop_words = set(line.strip() for line in f)\n",
    "        return stop_words\n",
    "    except FileNotFoundError:\n",
    "        print(f\"불용어 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return set()\n",
    "\n",
    "# 텍스트 전처리 함수 (간단한 정규식 기반 토크나이저 사용)\n",
    "def preprocess_text(text, stop_words):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # 단어 기준 토크나이저\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # 대괄호 내용 제거\n",
    "    text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "    tokens = tokenizer.tokenize(text)  # 텍스트를 단어 단위로 토크나이징\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # 불용어 제거\n",
    "    return tokens\n",
    "\n",
    "# 워드 클라우드 생성 함수\n",
    "def generate_wordcloud(title, tokens):\n",
    "    try:\n",
    "        # 단어 빈도 계산\n",
    "        freq = defaultdict(int)\n",
    "        for word in tokens:\n",
    "            freq[word] += 1\n",
    "        if not freq:\n",
    "            print(f\"{title} 워드 클라우드에 사용할 단어가 없습니다.\")\n",
    "            return\n",
    "        # 워드 클라우드 생성\n",
    "        wordcloud = WordCloud(\n",
    "            font_path=font_path,  # 한글 폰트 경로 설정\n",
    "            background_color='white',\n",
    "            width=800,\n",
    "            height=600\n",
    "        ).generate_from_frequencies(freq)\n",
    "        # 워드 클라우드 시각화\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.tight_layout(pad=0)\n",
    "        # 워드 클라우드를 이미지 파일로 저장\n",
    "        save_path = os.path.join(base_path, f\"{title}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"{title} 워드 클라우드 저장 완료: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{title} 워드 클라우드 생성 중 오류 발생: {e}\")\n",
    "\n",
    "# 데이터 로드 및 합치기\n",
    "def load_and_combine_data(file_names):\n",
    "    combined_texts = []\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(base_path, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for year_data in data:\n",
    "                    for entry in year_data.get(file_name.split('_')[2].split('.')[0], []):\n",
    "                        content = entry.get('content', '')\n",
    "                        combined_texts.append(content)\n",
    "        except FileNotFoundError as fnf_error:\n",
    "            print(f\"파일을 찾을 수 없습니다: {fnf_error}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{file_name} 처리 중 오류 발생: {e}\")\n",
    "    return combined_texts\n",
    "\n",
    "# 2020-2021년 데이터 로드 및 합치기\n",
    "file_names_2020_2021 = [\n",
    "    \"naver_blog_2020.json\",\n",
    "    \"naver_blog_2021.json\"\n",
    "]\n",
    "texts_2020_2021 = load_and_combine_data(file_names_2020_2021)\n",
    "\n",
    "# 2022-2024년 데이터 로드 및 합치기\n",
    "file_names_2022_2024 = [\n",
    "    \"naver_blog_2022.json\",\n",
    "    \"naver_blog_2023.json\",\n",
    "    \"naver_blog_2024.json\"\n",
    "]\n",
    "texts_2022_2024 = load_and_combine_data(file_names_2022_2024)\n",
    "\n",
    "# 전처리 및 워드 클라우드 생성\n",
    "def process_and_generate_wordcloud(texts, title, stopwords_path):\n",
    "    stop_words = load_stopwords(stopwords_path)\n",
    "    all_tokens = []\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text, stop_words)\n",
    "        all_tokens.extend(tokens)\n",
    "    generate_wordcloud(title, all_tokens)\n",
    "\n",
    "# 2020-2021년 워드 클라우드 생성\n",
    "process_and_generate_wordcloud(texts_2020_2021, \"2020-2021년\", stopwords_path)\n",
    "\n",
    "# 2022-2024년 워드 클라우드 생성\n",
    "process_and_generate_wordcloud(texts_2022_2024, \"2022-2024년\", stopwords_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
