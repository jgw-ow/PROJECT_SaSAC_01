{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 로드 성공: articles_20대_소비_2020.csv\n",
      "\n",
      "=== articles_20대_소비_2020.csv의 토픽 ===\n",
      "토픽 0: 0.002*\"코로나\" + 0.001*\"원장은\" + 0.001*\"강한\" + 0.001*\"물건을\" + 0.001*\"대에서는\"\n",
      "토픽 1: 0.003*\"코로나\" + 0.003*\"트위터\" + 0.003*\"SNS\" + 0.002*\"인스타그램\" + 0.001*\"경험이\"\n",
      "토픽 2: 0.003*\"코로나\" + 0.001*\"상하이지부장은\" + 0.001*\"무역협회\" + 0.001*\"뚜렷했다\" + 0.001*\"폐기됐다\"\n",
      "토픽 3: 0.004*\"코로나\" + 0.001*\"받으며\" + 0.001*\"가요\" + 0.001*\"증가했고\" + 0.001*\"동승관계\"\n",
      "토픽 4: 0.007*\"코로나\" + 0.002*\"MZ세대\" + 0.001*\"폭발적으로\" + 0.001*\"탈모\" + 0.001*\"매출액은\"\n",
      "파일 로드 성공: articles_20대_소비_2021.csv\n",
      "\n",
      "=== articles_20대_소비_2021.csv의 토픽 ===\n",
      "토픽 0: 0.001*\"지방선거\" + 0.001*\"원의\" + 0.001*\"SUV\" + 0.001*\"분석해보면\" + 0.001*\"수요도\"\n",
      "토픽 1: 0.005*\"MZ세대\" + 0.002*\"코로나\" + 0.002*\"히키코모리\" + 0.001*\"활동을\" + 0.001*\"제공\"\n",
      "토픽 2: 0.007*\"코로나\" + 0.001*\"응원\" + 0.001*\"플랫폼의\" + 0.001*\"카모아\" + 0.001*\"생활\"\n",
      "토픽 3: 0.003*\"코로나\" + 0.001*\"MZ세대인\" + 0.001*\"MZ세대가\" + 0.001*\"대표적인\" + 0.001*\"사이트\"\n",
      "토픽 4: 0.003*\"코로나\" + 0.002*\"활동을\" + 0.001*\"활동\" + 0.001*\"열심히\" + 0.001*\"못해\"\n",
      "파일 로드 성공: articles_20대_소비_2022.csv\n",
      "\n",
      "=== articles_20대_소비_2022.csv의 토픽 ===\n",
      "토픽 0: 0.001*\"끌었다\" + 0.001*\"것인가를\" + 0.001*\"본능이\" + 0.001*\"전망에서\" + 0.001*\"역사를\"\n",
      "토픽 1: 0.001*\"코로나\" + 0.001*\"알뜰폰이\" + 0.001*\"사건에\" + 0.001*\"MZ세대\" + 0.001*\"각본\"\n",
      "토픽 2: 0.007*\"코로나\" + 0.001*\"경험이\" + 0.001*\"소스\" + 0.001*\"치러진\" + 0.001*\"중반에서\"\n",
      "토픽 3: 0.003*\"활동을\" + 0.003*\"코로나\" + 0.003*\"여가활동\" + 0.002*\"활동\" + 0.001*\"회장이\"\n",
      "토픽 4: 0.003*\"MZ세대\" + 0.001*\"MZ세대가\" + 0.001*\"전략으로\" + 0.001*\"활용할\" + 0.001*\"내년부터\"\n",
      "파일 로드 성공: articles_20대_소비_2023.csv\n",
      "\n",
      "=== articles_20대_소비_2023.csv의 토픽 ===\n",
      "토픽 0: 0.001*\"보는\" + 0.001*\"친구\" + 0.001*\"극단적\" + 0.001*\"양산\" + 0.001*\"시성비\"\n",
      "토픽 1: 0.003*\"MZ가\" + 0.001*\"유기가공\" + 0.001*\"우울증\" + 0.001*\"패션과\" + 0.001*\"조원\"\n",
      "토픽 2: 0.001*\"현상이\" + 0.001*\"꺼리는\" + 0.001*\"줄어\" + 0.001*\"날씨가\" + 0.001*\"출신\"\n",
      "토픽 3: 0.001*\"등록\" + 0.001*\"맛있는\" + 0.001*\"쓰는\" + 0.001*\"마리당\" + 0.001*\"있기\"\n",
      "토픽 4: 0.001*\"먹어야\" + 0.001*\"Target\" + 0.001*\"예측된다\" + 0.001*\"걱정\" + 0.001*\"코로나\"\n",
      "파일 로드 성공: articles_20대_소비_2024.csv\n",
      "\n",
      "=== articles_20대_소비_2024.csv의 토픽 ===\n",
      "토픽 0: 0.009*\"경험은\" + 0.003*\"경험이\" + 0.001*\"인구는\" + 0.001*\"인스타그램\" + 0.001*\"여기어때\"\n",
      "토픽 1: 0.001*\"영향력을\" + 0.001*\"방송을\" + 0.001*\"부인하고\" + 0.001*\"일관한\" + 0.001*\"태도로\"\n",
      "토픽 2: 0.001*\"안에\" + 0.001*\"적은\" + 0.001*\"중점을\" + 0.001*\"들지\" + 0.001*\"비비\"\n",
      "토픽 3: 0.002*\"오호라가\" + 0.002*\"분석되고\" + 0.002*\"설치\" + 0.002*\"획득하는\" + 0.001*\"WINA\"\n",
      "토픽 4: 0.001*\"경험이\" + 0.001*\"MZ세대\" + 0.001*\"최우수\" + 0.001*\"국립\" + 0.001*\"국가사업으로\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_path = r\"C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\"\n",
    "stopwords_path = os.path.join(base_path, \"stopwords.txt\")\n",
    "\n",
    "# 불용어 파일 읽기 함수\n",
    "def load_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            stop_words = set(line.strip() for line in f)\n",
    "        return stop_words\n",
    "    except FileNotFoundError:\n",
    "        print(f\"불용어 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return set()\n",
    "\n",
    "# 텍스트 전처리 함수 (간단한 정규식 기반 토크나이저 사용)\n",
    "def preprocess_text(text, stopwords_path):\n",
    "    stop_words = load_stopwords(stopwords_path)  # 최신 불용어 목록 로드\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # 단어 기준 토크나이저\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # 대괄호 내용 제거\n",
    "    text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "    tokens = tokenizer.tokenize(text)  # 텍스트를 단어 단위로 토크나이징\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # 불용어 제거\n",
    "    return tokens\n",
    "\n",
    "# LDA 분석 함수\n",
    "def lda_analysis(file_name, stopwords_path, num_topics=5, passes=10):\n",
    "    try:\n",
    "        # 파일 경로 설정\n",
    "        file_path = os.path.join(base_path, file_name)\n",
    "        \n",
    "        # 데이터 로드\n",
    "        data = pd.read_csv(file_path, encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "        print(f\"파일 로드 성공: {file_name}\")\n",
    "\n",
    "        # 데이터 컬럼 이름을 소문자로 변환\n",
    "        data.columns = data.columns.str.lower()\n",
    "\n",
    "        # 'description' 컬럼 전처리\n",
    "        data['description'] = data['description'].fillna('')  # 결측값 처리\n",
    "        data['processed'] = data['description'].apply(lambda text: preprocess_text(text, stopwords_path))\n",
    "\n",
    "        # 말뭉치 및 사전 생성\n",
    "        dictionary = corpora.Dictionary(data['processed'])\n",
    "        corpus = [dictionary.doc2bow(text) for text in data['processed']]\n",
    "\n",
    "        # LDA 모델 학습\n",
    "        lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "\n",
    "        # 토픽 출력\n",
    "        print(f\"\\n=== {file_name}의 토픽 ===\")\n",
    "        topics = lda_model.print_topics(num_words=5)\n",
    "        for idx, topic in topics:\n",
    "            print(f\"토픽 {idx}: {topic}\")\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"파일을 찾을 수 없습니다: {fnf_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{file_name} 처리 중 오류 발생: {e}\")\n",
    "\n",
    "# 5개년 데이터 처리\n",
    "file_names = [f\"articles_20대_소비_{year}.csv\" for year in range(2020, 2025)]\n",
    "for file_name in file_names:\n",
    "    lda_analysis(file_name, stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from wordcloud) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from wordcloud) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl (301 kB)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 데이터 로드 시작 ===\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2020.csv\n",
      "articles_20대_소비_2020.csv 데이터를 그룹1에 추가했습니다.\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2021.csv\n",
      "articles_20대_소비_2021.csv 데이터를 그룹1에 추가했습니다.\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2022.csv\n",
      "articles_20대_소비_2022.csv 데이터를 그룹2에 추가했습니다.\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2023.csv\n",
      "articles_20대_소비_2023.csv 데이터를 그룹2에 추가했습니다.\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2024.csv\n",
      "articles_20대_소비_2024.csv 데이터를 그룹2에 추가했습니다.\n",
      "\n",
      "=== 데이터 로드 완료 ===\n",
      "그룹1 (2020-2021년) 데이터 수: 1400\n",
      "그룹2 (2022-2024년) 데이터 수: 2128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\host\\AppData\\Local\\Temp\\ipykernel_33768\\3654398660.py:60: UserWarning: Glyph 45380 (\\N{HANGUL SYLLABLE NYEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(pad=0)\n",
      "C:\\Users\\host\\AppData\\Local\\Temp\\ipykernel_33768\\3654398660.py:63: UserWarning: Glyph 45380 (\\N{HANGUL SYLLABLE NYEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-2021년 워드 클라우드 저장 완료: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\2020-2021년.png\n",
      "2022-2024년 워드 클라우드 저장 완료: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\2022-2024년.png\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_path = r\"C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\"\n",
    "stopwords_path = os.path.join(base_path, \"stopwords.txt\")\n",
    "\n",
    "# 불용어 파일 읽기 함수\n",
    "def load_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            stop_words = set(line.strip() for line in f)\n",
    "        return stop_words\n",
    "    except FileNotFoundError:\n",
    "        print(f\"불용어 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return set()\n",
    "\n",
    "# 텍스트 전처리 함수 (간단한 정규식 기반 토크나이저 사용)\n",
    "def preprocess_text(text, stopwords_path):\n",
    "    stop_words = load_stopwords(stopwords_path)  # 최신 불용어 목록 로드\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # 단어 기준 토크나이저\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # 대괄호 내용 제거\n",
    "    text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "    tokens = tokenizer.tokenize(text)  # 텍스트를 단어 단위로 토크나이징\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # 불용어 제거\n",
    "    return tokens\n",
    "\n",
    "# 워드 클라우드 생성 함수\n",
    "def generate_wordcloud(title, texts):\n",
    "    try:\n",
    "        # 모든 텍스트를 하나로 합침\n",
    "        all_text = ' '.join(texts)\n",
    "        # 전처리\n",
    "        tokens = preprocess_text(all_text, stopwords_path)\n",
    "        # 단어 빈도 계산\n",
    "        freq = defaultdict(int)\n",
    "        for word in tokens:\n",
    "            freq[word] += 1\n",
    "        if not freq:\n",
    "            print(f\"{title} 워드 클라우드에 사용할 단어가 없습니다.\")\n",
    "            return\n",
    "        # 워드 클라우드 생성\n",
    "        wordcloud = WordCloud(\n",
    "            font_path=os.path.join(base_path, \"NanumGothic.ttf\"),  # 나눔 고딕 폰트 경로 (사용자 환경에 맞게 수정)\n",
    "            background_color='white',\n",
    "            width=800,\n",
    "            height=600\n",
    "        ).generate_from_frequencies(freq)\n",
    "        # 워드 클라우드 시각화\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.tight_layout(pad=0)\n",
    "        # 워드 클라우드를 이미지 파일로 저장\n",
    "        save_path = os.path.join(base_path, f\"{title}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"{title} 워드 클라우드 저장 완료: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{title} 워드 클라우드 생성 중 오류 발생: {e}\")\n",
    "\n",
    "# 데이터 로드 및 그룹화\n",
    "def load_and_group_data(file_names, group1_years, group2_years):\n",
    "    group1_data = []\n",
    "    group2_data = []\n",
    "    print(\"\\n=== 데이터 로드 시작 ===\")\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(base_path, file_name)\n",
    "        print(f\"\\n파일 로드: {file_path}\")\n",
    "        data = pd.read_csv(file_path, encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "        if data is not None:\n",
    "            # 데이터 컬럼 이름을 소문자로 변환 (문자열만)\n",
    "            data.columns = [col.lower() if isinstance(col, str) else col for col in data.columns]\n",
    "            # 텍스트 컬럼 자동 탐색\n",
    "            text_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "            if not text_columns:\n",
    "                print(f\"문자열 데이터가 있는 컬럼이 {file_name}에 존재하지 않습니다.\")\n",
    "                continue\n",
    "            # 우선 'description' 컬럼이 있으면 사용, 없으면 첫 번째 문자열 컬럼 사용\n",
    "            if 'description' in text_columns:\n",
    "                text_col = 'description'\n",
    "            else:\n",
    "                text_col = text_columns[0]\n",
    "                print(f\"'description' 컬럼이 {file_name}에 존재하지 않습니다. 대신 '{text_col}' 컬럼을 사용합니다.\")\n",
    "            # 선택된 컬럼의 데이터를 문자열로 변환\n",
    "            data[text_col] = data[text_col].apply(lambda x: str(x) if not isinstance(x, str) else x).fillna('')\n",
    "            # 문서 추출\n",
    "            documents = data[text_col].tolist()\n",
    "            if not documents:\n",
    "                print(f\"{file_name}에서 문서를 추출하지 못했습니다.\")\n",
    "                continue\n",
    "            # 그룹에 따라 데이터 추가\n",
    "            year_num = re.findall(r'\\d{4}', file_name)[0]\n",
    "            if year_num in group1_years:\n",
    "                group1_data.extend(documents)\n",
    "                print(f\"{file_name} 데이터를 그룹1에 추가했습니다.\")\n",
    "            elif year_num in group2_years:\n",
    "                group2_data.extend(documents)\n",
    "                print(f\"{file_name} 데이터를 그룹2에 추가했습니다.\")\n",
    "            else:\n",
    "                print(f\"{file_name}은 정의된 그룹에 속하지 않습니다.\")\n",
    "        else:\n",
    "            print(f\"{file_name} 데이터 로드에 실패했습니다.\")\n",
    "    # 데이터 확인\n",
    "    print(\"\\n=== 데이터 로드 완료 ===\")\n",
    "    print(f\"그룹1 (2020-2021년) 데이터 수: {len(group1_data)}\")\n",
    "    print(f\"그룹2 (2022-2024년) 데이터 수: {len(group2_data)}\")\n",
    "    return group1_data, group2_data\n",
    "\n",
    "# 5개년 데이터 처리\n",
    "file_names = [f\"articles_20대_소비_{year}.csv\" for year in range(2020, 2025)]\n",
    "group1_years = ['2020', '2021']\n",
    "group2_years = ['2022', '2023', '2024']\n",
    "\n",
    "group1_data, group2_data = load_and_group_data(file_names, group1_years, group2_years)\n",
    "\n",
    "# 워드 클라우드 생성 실행\n",
    "if group1_data:\n",
    "    generate_wordcloud(\"2020-2021년\", group1_data)\n",
    "else:\n",
    "    print(\"그룹1에 데이터가 없습니다.\")\n",
    "if group2_data:\n",
    "    generate_wordcloud(\"2022-2024년\", group2_data)\n",
    "else:\n",
    "    print(\"그룹2에 데이터가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 데이터 로드 시작 ===\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2020.csv\n",
      "articles_20대_소비_2020.csv 데이터를 그룹1에 추가했습니다.\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2021.csv\n",
      "articles_20대_소비_2021.csv 데이터를 그룹1에 추가했습니다.\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2022.csv\n",
      "articles_20대_소비_2022.csv 데이터를 그룹2에 추가했습니다.\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2023.csv\n",
      "articles_20대_소비_2023.csv 데이터를 그룹2에 추가했습니다.\n",
      "\n",
      "파일 로드: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\articles_20대_소비_2024.csv\n",
      "articles_20대_소비_2024.csv 데이터를 그룹2에 추가했습니다.\n",
      "\n",
      "=== 데이터 로드 완료 ===\n",
      "그룹1 (2020-2021년) 데이터 수: 1400\n",
      "그룹2 (2022-2024년) 데이터 수: 2128\n",
      "2020-2021년 워드 클라우드 저장 완료: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\2020-2021년.png\n",
      "2022-2024년 워드 클라우드 저장 완료: C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\\2022-2024년.png\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import chardet\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_path = r\"C:\\Users\\host\\Desktop\\PROJECT_SaSAC_01\\PROJECT_SaSAC_01\"\n",
    "stopwords_path = os.path.join(base_path, \"stopwords.txt\")\n",
    "\n",
    "# 한글 폰트 설정\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"  # Windows의 경우\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "# 불용어 파일 읽기 함수\n",
    "def load_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            stop_words = set(line.strip() for line in f)\n",
    "        return stop_words\n",
    "    except FileNotFoundError:\n",
    "        print(f\"불용어 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return set()\n",
    "\n",
    "# 텍스트 전처리 함수 (간단한 정규식 기반 토크나이저 사용)\n",
    "def preprocess_text(text, stopwords_path):\n",
    "    stop_words = load_stopwords(stopwords_path)  # 최신 불용어 목록 로드\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # 단어 기준 토크나이저\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # 대괄호 내용 제거\n",
    "    text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "    tokens = tokenizer.tokenize(text)  # 텍스트를 단어 단위로 토크나이징\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # 불용어 제거\n",
    "    return tokens\n",
    "\n",
    "# 워드 클라우드 생성 함수\n",
    "def generate_wordcloud(title, texts):\n",
    "    try:\n",
    "        # 모든 텍스트를 하나로 합침\n",
    "        all_text = ' '.join(texts)\n",
    "        # 전처리\n",
    "        tokens = preprocess_text(all_text, stopwords_path)\n",
    "        # 단어 빈도 계산\n",
    "        freq = defaultdict(int)\n",
    "        for word in tokens:\n",
    "            freq[word] += 1\n",
    "        if not freq:\n",
    "            print(f\"{title} 워드 클라우드에 사용할 단어가 없습니다.\")\n",
    "            return\n",
    "        # 워드 클라우드 생성\n",
    "        wordcloud = WordCloud(\n",
    "            font_path=font_path,  # 한글 폰트 경로 설정\n",
    "            background_color='white',\n",
    "            width=800,\n",
    "            height=600\n",
    "        ).generate_from_frequencies(freq)\n",
    "        # 워드 클라우드 시각화\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.tight_layout(pad=0)\n",
    "        # 워드 클라우드를 이미지 파일로 저장\n",
    "        save_path = os.path.join(base_path, f\"{title}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"{title} 워드 클라우드 저장 완료: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{title} 워드 클라우드 생성 중 오류 발생: {e}\")\n",
    "\n",
    "# 파일 인코딩 감지 함수\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "# 데이터 로드 및 그룹화\n",
    "def load_and_group_data(file_names, group1_years, group2_years):\n",
    "    group1_data = []\n",
    "    group2_data = []\n",
    "    print(\"\\n=== 데이터 로드 시작 ===\")\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(base_path, file_name)\n",
    "        print(f\"\\n파일 로드: {file_path}\")\n",
    "        encoding = detect_encoding(file_path)\n",
    "        data = pd.read_csv(file_path, encoding=encoding, on_bad_lines='skip', engine='python')\n",
    "        if data is not None:\n",
    "            # 데이터 컬럼 이름을 소문자로 변환 (문자열만)\n",
    "            data.columns = [col.lower() if isinstance(col, str) else col for col in data.columns]\n",
    "            # 텍스트 컬럼 자동 탐색\n",
    "            text_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "            if not text_columns:\n",
    "                print(f\"문자열 데이터가 있는 컬럼이 {file_name}에 존재하지 않습니다.\")\n",
    "                continue\n",
    "            # 우선 'description' 컬럼이 있으면 사용, 없으면 첫 번째 문자열 컬럼 사용\n",
    "            if 'description' in text_columns:\n",
    "                text_col = 'description'\n",
    "            else:\n",
    "                text_col = text_columns[0]\n",
    "                print(f\"'description' 컬럼이 {file_name}에 존재하지 않습니다. 대신 '{text_col}' 컬럼을 사용합니다.\")\n",
    "            # 선택된 컬럼의 데이터를 문자열로 변환\n",
    "            data[text_col] = data[text_col].apply(lambda x: str(x) if not isinstance(x, str) else x).fillna('')\n",
    "            # 문서 추출\n",
    "            documents = data[text_col].tolist()\n",
    "            if not documents:\n",
    "                print(f\"{file_name}에서 문서를 추출하지 못했습니다.\")\n",
    "                continue\n",
    "            # 그룹에 따라 데이터 추가\n",
    "            year_num = re.findall(r'\\d{4}', file_name)[0]\n",
    "            if year_num in group1_years:\n",
    "                group1_data.extend(documents)\n",
    "                print(f\"{file_name} 데이터를 그룹1에 추가했습니다.\")\n",
    "            elif year_num in group2_years:\n",
    "                group2_data.extend(documents)\n",
    "                print(f\"{file_name} 데이터를 그룹2에 추가했습니다.\")\n",
    "            else:\n",
    "                print(f\"{file_name}은 정의된 그룹에 속하지 않습니다.\")\n",
    "        else:\n",
    "            print(f\"{file_name} 데이터 로드에 실패했습니다.\")\n",
    "    # 데이터 확인\n",
    "    print(\"\\n=== 데이터 로드 완료 ===\")\n",
    "    print(f\"그룹1 (2020-2021년) 데이터 수: {len(group1_data)}\")\n",
    "    print(f\"그룹2 (2022-2024년) 데이터 수: {len(group2_data)}\")\n",
    "    return group1_data, group2_data\n",
    "\n",
    "# 5개년 데이터 처리\n",
    "file_names = [f\"articles_20대_소비_{year}.csv\" for year in range(2020, 2025)]\n",
    "group1_years = ['2020', '2021']\n",
    "group2_years = ['2022', '2023', '2024']\n",
    "\n",
    "group1_data, group2_data = load_and_group_data(file_names, group1_years, group2_years)\n",
    "\n",
    "# 워드 클라우드 생성 실행\n",
    "if group1_data:\n",
    "    generate_wordcloud(\"2020-2021년\", group1_data)\n",
    "else:\n",
    "    print(\"그룹1에 데이터가 없습니다.\")\n",
    "if group2_data:\n",
    "    generate_wordcloud(\"2022-2024년\", group2_data)\n",
    "else:\n",
    "    print(\"그룹2에 데이터가 없습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
