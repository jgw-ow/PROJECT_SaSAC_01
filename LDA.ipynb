{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import re\n",
    "# 불용어 목록 정의\n",
    "stop_words = {'수', '이', '그', '등', '더', '만', '들', '것', '같', '되', '있', '한', '티르티르', 'pzp', '있어요'}\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # 대괄호 내용 제거\n",
    "    text = re.sub(r'\\W+', ' ', text)  # 특수 문자 제거\n",
    "    tokens = text.lower().split()  # 공백 기준으로 토큰화\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # 불용어 제거\n",
    "    return tokens\n",
    "\n",
    "# `content` 컬럼 전처리\n",
    "data['processed'] = data['content'].apply(preprocess_text)\n",
    "\n",
    "# 말뭉치 및 사전 생성\n",
    "dictionary = corpora.Dictionary(data['processed'])\n",
    "corpus = [dictionary.doc2bow(text) for text in data['processed']]\n",
    "\n",
    "# LDA 모델 학습\n",
    "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=2)\n",
    "\n",
    "# 토픽 출력\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for idx, topic in topics:\n",
    "    print(f\"토픽 {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_path = r\"C:\\Users\\SJ\\Desktop\\새싹_종로\\파이썬\"\n",
    "stopwords_path = os.path.join(base_path, \"stopwords.txt\")\n",
    "\n",
    "# 불용어 파일 읽기 함수\n",
    "def load_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            stop_words = set(line.strip() for line in f)\n",
    "        return stop_words\n",
    "    except FileNotFoundError:\n",
    "        print(f\"불용어 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return set()\n",
    "\n",
    "# 텍스트 전처리 함수 (간단한 정규식 기반 토크나이저 사용)\n",
    "def preprocess_text(text, stopwords_path):\n",
    "    stop_words = load_stopwords(stopwords_path)  # 최신 불용어 목록 로드\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # 단어 기준 토크나이저\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # 대괄호 내용 제거\n",
    "    tokens = tokenizer.tokenize(text)  # 텍스트를 단어 단위로 토크나이징\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # 불용어 제거\n",
    "    return tokens\n",
    "\n",
    "# LDA 분석 함수\n",
    "def lda_analysis(file_name, stopwords_path, num_topics=5, passes=10):\n",
    "    try:\n",
    "        # 파일 경로 설정\n",
    "        file_path = os.path.join(base_path, file_name)\n",
    "        \n",
    "        # 데이터 로드\n",
    "        data = pd.read_csv(file_path, encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "        print(f\"파일 로드 성공: {file_name}\")\n",
    "\n",
    "        # 데이터 컬럼 이름을 소문자로 변환\n",
    "        data.columns = data.columns.str.lower()\n",
    "\n",
    "        # 'description' 컬럼 전처리\n",
    "        data['description'] = data['description'].fillna('')  # 결측값 처리\n",
    "        data['processed'] = data['description'].apply(lambda text: preprocess_text(text, stopwords_path))\n",
    "\n",
    "        # 말뭉치 및 사전 생성\n",
    "        dictionary = corpora.Dictionary(data['processed'])\n",
    "        corpus = [dictionary.doc2bow(text) for text in data['processed']]\n",
    "\n",
    "        # LDA 모델 학습\n",
    "        lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "\n",
    "        # 토픽 출력\n",
    "        print(f\"\\n=== {file_name}의 토픽 ===\")\n",
    "        topics = lda_model.print_topics(num_words=5)\n",
    "        for idx, topic in topics:\n",
    "            print(f\"토픽 {idx}: {topic}\")\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"파일을 찾을 수 없습니다: {fnf_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{file_name} 처리 중 오류 발생: {e}\")\n",
    "\n",
    "# 5개년 데이터 처리\n",
    "file_names = [f\"articles_{year}.csv\" for year in range(2020, 2025)]\n",
    "for file_name in file_names:\n",
    "    lda_analysis(file_name, stopwords_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
