{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from requests->webdriver-manager) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from requests->webdriver-manager) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from requests->webdriver-manager) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "\n",
    "# 웹드라이버 설정\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "# 버전에 상관 없이 os에 설치된 크롬 브라우저 사용\n",
    "driver = webdriver.Chrome(service=webdriver.chrome.service.Service(ChromeDriverManager().install()), options=options)\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "# Naver API key 입력\n",
    "client_id = '' \n",
    "client_secret = ''\n",
    "\n",
    "# selenium으로 검색 페이지 불러오기 #\n",
    "naver_urls = []\n",
    "postdate = []\n",
    "titles = []\n",
    "\n",
    "# 검색어 입력\n",
    "keywords = input(\"검색할 키워드를 입력해주세요 (쉼표로 구분):\")\n",
    "keyword_list = keywords.split(',')\n",
    "encText = urllib.parse.quote(' '.join(keyword_list))\n",
    "\n",
    "# 검색을 끝낼 페이지 입력\n",
    "end = input(\"\\n크롤링을 끝낼 위치를 입력해주세요. (기본값:1, 최대값:100):\")  \n",
    "if end == \"\":\n",
    "    end = 1\n",
    "else:\n",
    "    end = int(end)\n",
    "print(\"\\n 1 ~ \", end, \"페이지 까지 크롤링을 진행 합니다\")\n",
    "\n",
    "# 한번에 가져올 페이지 입력\n",
    "display = input(\"\\n한번에 가져올 페이지 개수를 입력해주세요.(기본값:10, 최대값: 100):\")\n",
    "if display == \"\":\n",
    "    display = 10\n",
    "else:\n",
    "    display = int(display)\n",
    "print(\"\\n한번에 가져올 페이지 : \", display, \"페이지\")\n",
    "\n",
    "for start in range(1, end * display + 1, display):\n",
    "    url = f\"https://openapi.naver.com/v1/search/blog?query={encText}&start={start}&display={display}\"  # JSON 결과\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "    if rescode == 200:\n",
    "        response_body = response.read()\n",
    "        \n",
    "        data = json.loads(response_body.decode('utf-8'))['items']\n",
    "        for row in data:\n",
    "            if 'blog.naver' in row['link']:\n",
    "                naver_urls.append(row['link'])\n",
    "                postdate.append(row['postdate'])\n",
    "                title = row['title']\n",
    "                # html태그제거\n",
    "                pattern1 = '<[^>]*>'\n",
    "                title = re.sub(pattern=pattern1, repl='', string=title)\n",
    "                titles.append(title)\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        print(\"Error Code:\" + rescode)\n",
    "\n",
    "###naver 기사 본문 및 제목 가져오기###\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "contents = []\n",
    "comments_texts = []\n",
    "try:\n",
    "    for i in naver_urls:\n",
    "        print(i)\n",
    "        driver.get(i)\n",
    "        time.sleep(5)  # 대기시간 변경 가능\n",
    "\n",
    "        iframe = driver.find_element(By.ID , \"mainFrame\") # id가 mainFrame이라는 요소를 찾아내고 -> iframe임\n",
    "        driver.switch_to.frame(iframe) # 이 iframe이 내가 찾고자하는 html을 포함하고 있는 내용\n",
    "\n",
    "        source = driver.page_source\n",
    "        html = BeautifulSoup(source, \"html.parser\")\n",
    "        \n",
    "        # 기사 텍스트만 가져오기\n",
    "        content = html.select(\"div.se-main-container\")\n",
    "        #  list합치기\n",
    "        content = ''.join(str(content))\n",
    "\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = content.replace(pattern2, '')\n",
    "        content = content.replace('\\n', '')\n",
    "        content = content.replace('\\u200b', '')\n",
    "        contents.append(content)\n",
    "\n",
    "    news_df = pd.DataFrame({'title': titles, 'content': contents, 'date': postdate})\n",
    "    news_df.to_csv('blog.csv', index=False, encoding='utf-8-sig')\n",
    "except:\n",
    "    contents.append('error')\n",
    "    news_df = pd.DataFrame({'title': titles, 'content': contents, 'date': postdate})\n",
    "    news_df.to_csv('blog.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024년 12월 성수 팝업 - tvN 홀리데이 파티 (팝업 기간은...</td>\n",
       "      <td>[안녕하세여! 여러분~!  여러분~ 2025년 되었습니다!!  새해 복 많이 받으세...</td>\n",
       "      <td>20250101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[성수] 모푸샌드 팝업 스토어, 12월 마무리는 귀여운...</td>\n",
       "      <td>[ 예삐 학교가 12월 26, 27일을 재량 휴업일로 지정하는 바람에 휴가를 쓸 수...</td>\n",
       "      <td>20241227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12월 2주차 주간일기4 - 모푸샌드 mofusand 성수 팝업...</td>\n",
       "      <td>[불과 3년 전까지만 해도 우리 집은한 달에 한두 번은 쇼핑을 하러이마트나 코스트코...</td>\n",
       "      <td>20250102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유니클로 팝업 성수 히트텍 요원 되기?</td>\n",
       "      <td>[한때는 불매운동도 하더만 점점 이미지 좋아져서(?)성수 팝업도 오픈했어요하긴 겨울...</td>\n",
       "      <td>20241116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12월 성수 팝업 스토어 르쿠르제 팝업 예약없이 방문후기 (ft....</td>\n",
       "      <td>[    12월 성수 팝업스토어 일정 중 제일 핫한 르쿠르제 팝업  &amp;lt;Unfo...</td>\n",
       "      <td>20241206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>성수 팝업 핫플 추천, 빼빼로 미니 팝업 이벤트 참여해요!</td>\n",
       "      <td>[ 다양한 팝업스토어 구경하는 재미가 잇는성수동에서 이번에 빼빼로 팝업스토어가 오픈...</td>\n",
       "      <td>20241020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>뉴발란스 성수 993 팝업스토어 오픈, 아트갤러리 방문기</td>\n",
       "      <td>[본 포스팅은 ‘뉴발란스’ 로부터 소정의 원고료를 제공받아 솔직하게 작성된 후기입니...</td>\n",
       "      <td>20241017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>퍼셀 긱 사이언티스트 랩 성수 팝업(1등 당첨 방문후기)</td>\n",
       "      <td>[퍼셀을 알게 된건 신세계면세점!퍼셀 XXL 메이크오버 스틱 구매해서 써봤었는데코옆...</td>\n",
       "      <td>20241024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>11월 성수 팝업, 핫플 돌다가... 패딩 더현대서울팝업 정보 겟</td>\n",
       "      <td>[본 포스팅은 듀베티카로부터소정의 원고료를 제공받아 작성되었습니다 요즘 특히 핫한 ...</td>\n",
       "      <td>20241110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>10월 성수 팝업 본품 주는 루나 베이스 챔피언십 팝업스토어</td>\n",
       "      <td>[누구나 본품 받을 수 있는 10월 성수 팝업 루나 베이스 챔피언십 팝업스토어  안...</td>\n",
       "      <td>20241024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "0    2024년 12월 성수 팝업 - tvN 홀리데이 파티 (팝업 기간은...    \n",
       "1           [성수] 모푸샌드 팝업 스토어, 12월 마무리는 귀여운...    \n",
       "2      12월 2주차 주간일기4 - 모푸샌드 mofusand 성수 팝업...    \n",
       "3                        유니클로 팝업 성수 히트텍 요원 되기?   \n",
       "4     12월 성수 팝업 스토어 르쿠르제 팝업 예약없이 방문후기 (ft....    \n",
       "..                                         ...   \n",
       "495           성수 팝업 핫플 추천, 빼빼로 미니 팝업 이벤트 참여해요!   \n",
       "496            뉴발란스 성수 993 팝업스토어 오픈, 아트갤러리 방문기   \n",
       "497            퍼셀 긱 사이언티스트 랩 성수 팝업(1등 당첨 방문후기)   \n",
       "498       11월 성수 팝업, 핫플 돌다가... 패딩 더현대서울팝업 정보 겟   \n",
       "499          10월 성수 팝업 본품 주는 루나 베이스 챔피언십 팝업스토어   \n",
       "\n",
       "                                               content      date  \n",
       "0    [안녕하세여! 여러분~!  여러분~ 2025년 되었습니다!!  새해 복 많이 받으세...  20250101  \n",
       "1    [ 예삐 학교가 12월 26, 27일을 재량 휴업일로 지정하는 바람에 휴가를 쓸 수...  20241227  \n",
       "2    [불과 3년 전까지만 해도 우리 집은한 달에 한두 번은 쇼핑을 하러이마트나 코스트코...  20250102  \n",
       "3    [한때는 불매운동도 하더만 점점 이미지 좋아져서(?)성수 팝업도 오픈했어요하긴 겨울...  20241116  \n",
       "4    [    12월 성수 팝업스토어 일정 중 제일 핫한 르쿠르제 팝업  &lt;Unfo...  20241206  \n",
       "..                                                 ...       ...  \n",
       "495  [ 다양한 팝업스토어 구경하는 재미가 잇는성수동에서 이번에 빼빼로 팝업스토어가 오픈...  20241020  \n",
       "496  [본 포스팅은 ‘뉴발란스’ 로부터 소정의 원고료를 제공받아 솔직하게 작성된 후기입니...  20241017  \n",
       "497  [퍼셀을 알게 된건 신세계면세점!퍼셀 XXL 메이크오버 스틱 구매해서 써봤었는데코옆...  20241024  \n",
       "498  [본 포스팅은 듀베티카로부터소정의 원고료를 제공받아 작성되었습니다 요즘 특히 핫한 ...  20241110  \n",
       "499  [누구나 본품 받을 수 있는 10월 성수 팝업 루나 베이스 챔피언십 팝업스토어  안...  20241024  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('blog500.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# 웹 드라이버 선택\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 웹 드라이버 실행\n",
    "driver.get('https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query=%EC%84%B1%EC%88%98+%ED%8C%9D%EC%97%85')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "셀레니움으로 사이트 주소 리스트를 저장하는 과정입니다.\n",
    "get_site 함수는 사이트 각 설정 값 적용 및 게시물별 주소를 저장합니다.\n",
    "\"\"\"\n",
    "# 사이트 주소 담을 리스트\n",
    "site_list = []\n",
    "\n",
    "# 옵션버튼 최초 1회만 클릭하기 위한 변수\n",
    "option_button = 0\n",
    "\n",
    "def get_site(year, size):\n",
    "    # 옵션버튼 최초 1회만 클릭하도록 설정\n",
    "    global option_button\n",
    "    if option_button == 0:\n",
    "        driver.find_element(By.CSS_SELECTOR, 'i.spnew.ico_filter_arr').click()\n",
    "        option_button += 1\n",
    "        driver.implicitly_wait(3)\n",
    "\n",
    "    # 기간 선택 화살표 클릭\n",
    "    driver.find_element(By.CSS_SELECTOR, 'i.spnew.ico_check').click()\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    # 시작하는 날짜 선택\n",
    "    driver.find_element(By.CSS_SELECTOR, 'a.spnew_bf.ico_calendar._start_trigger').click()\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    # year년 1월 1일 선택\n",
    "    driver.find_element(By.XPATH, f'//div[@class=\"group_select _list_root\"]//strong[contains(text(), \"년(Year)\")]/following-sibling::div[@class=\"select_cont\"]//ul[@class=\"lst_item _ul\"]/li[@data-value=\"{year}\"]').click()\n",
    "    driver.implicitly_wait(3)\n",
    "    driver.find_element(By.XPATH, '//div[@class=\"group_select _list_root\"]//strong[contains(text(), \"월(Month)\")]/following-sibling::div[@class=\"select_cont\"]//ul[@class=\"lst_item _ul\"]/li[@data-value=\"1\"]').click()\n",
    "    driver.implicitly_wait(3)\n",
    "    driver.find_element(By.XPATH, '//div[@class=\"group_select _list_root\"]//strong[contains(text(), \"일(Day)\")]/following-sibling::div[@class=\"select_cont\"]//ul[@class=\"lst_item _ul\"]/li[@data-value=\"1\"]').click()\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    # 끝나는 날짜 선택\n",
    "    driver.find_element(By.CSS_SELECTOR, 'a.spnew_bf.ico_calendar._end_trigger').click()\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    # year년 12월 31일 선택\n",
    "    driver.find_element(By.XPATH, f'//div[@class=\"group_select _list_root\"]//strong[contains(text(), \"년(Year)\")]/following-sibling::div[@class=\"select_cont\"]//ul[@class=\"lst_item _ul\"]/li[@data-value=\"{year}\"]').click()\n",
    "    driver.implicitly_wait(3)\n",
    "    driver.find_element(By.XPATH, '//div[@class=\"group_select _list_root\"]//strong[contains(text(), \"월(Month)\")]/following-sibling::div[@class=\"select_cont\"]//ul[@class=\"lst_item _ul\"]/li[@data-value=\"12\"]').click()\n",
    "    driver.implicitly_wait(3)\n",
    "    driver.find_element(By.XPATH, '//div[@class=\"group_select _list_root\"]//strong[contains(text(), \"일(Day)\")]/following-sibling::div[@class=\"select_cont\"]//ul[@class=\"lst_item _ul\"]/li[@data-value=\"31\"]').click()\n",
    "    driver.implicitly_wait(3)\n",
    "    \n",
    "    # 적용 버튼 클릭\n",
    "    driver.find_element(By.CSS_SELECTOR, 'button.btn_apply._apply_btn').click()\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    # 1000개 이상이 될때까지 스크롤\n",
    "    while len(driver.find_elements(By.CSS_SELECTOR, 'a.title_link')) < size:\n",
    "        driver.find_element(By.CSS_SELECTOR, 'body').send_keys(Keys.END)\n",
    "        driver.find_element(By.CSS_SELECTOR, 'body').send_keys(Keys.END)\n",
    "        driver.find_element(By.CSS_SELECTOR, 'body').send_keys(Keys.END)\n",
    "        driver.implicitly_wait(3)\n",
    "\n",
    "    # 진행 상황 확인을 위해 글로벌 변수화(몰라도 됨)\n",
    "    global href_list\n",
    "\n",
    "    # 개별 사이트 주소를 리스트에 저장하고, 1000개 넘어갈 경우 제거\n",
    "    href_list = [i.get_attribute('href') for i in driver.find_elements(By.CSS_SELECTOR, 'a.title_link')][:size]\n",
    "    site_list.extend(href_list)\n",
    "\n",
    "    # 날짜 설정 변경을 위해 가장 상단으로 스크롤 이동\n",
    "    driver.find_element(By.CSS_SELECTOR, 'body').send_keys(Keys.HOME)\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "수집된 개별 사이트 주소를 이용해서 게시날짜, 제목, 본문을 저장하는 과정입니다.\n",
    "개별 사이트는 iframe이라는 형식으로 되어 있습니다.\n",
    "따라서 셀레니움이 해당사이트에 방문 후 iframe에 진입해야 크롤링이 가능합니다.\n",
    "\"\"\" \n",
    "# 본문 담을 리스트\n",
    "content_list = []\n",
    "\n",
    "# 년도별 데이터 담을 딕셔너리\n",
    "year_class = {}\n",
    "\n",
    "# selenium으로 iframe에 들어가서 본문 긁어오기 함수\n",
    "def get_content(url):\n",
    "    # 사이트 방문\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    # iframe영역으로 들어가기\n",
    "    driver.switch_to.frame('mainFrame')\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    # 날짜\n",
    "    dates = driver.find_element(By.CSS_SELECTOR, 'span.se_publishDate.pcol2').text.strip()\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    # 제목\n",
    "    title = driver.find_element(By.CSS_SELECTOR, 'div.se-module.se-module-text.se-title-text').text.strip()\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    # 본문\n",
    "    content = [i.text for i in driver.find_elements(By.CSS_SELECTOR, 'div.se-component.se-text.se-l-default')]\n",
    "    content = ''.join(content).strip().replace('\\n', ' ')\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    content_dict = {\"date\" : dates,\n",
    "                    \"title\" :title,\n",
    "                    \"content\" : content}\n",
    "\n",
    "    # 년도만 출력\n",
    "    year = content_dict['date'].split('.')[0]\n",
    "    \n",
    "    # {년도 : content_dict}틀 만들기 \n",
    "    if year not in year_class:\n",
    "        year_class[year] = []\n",
    "    \n",
    "    # 년도별 딕셔너리 형태로 저장\n",
    "    year_class[year].append(content_dict)\n",
    "    \n",
    "    ## iframe 영역 나가기\n",
    "    # driver.switch_to.default_content() \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "앞서 만든 get_site함수와 get_content함수를 통합한 함수입니다.\n",
    "해당 함수에 \n",
    "year_list = 수집할 연도 리스트\n",
    "year_size = 연도별 글 개수\n",
    "이 두가지만 입력하면 자동으로 돌아갑니다.\n",
    "\"\"\"\n",
    "\n",
    "# 연도별, 주소 수집 함수\n",
    "def get_json(year_list, year_size):\n",
    "    for i in tqdm(year_list):\n",
    "        get_site(i, year_size)\n",
    "        print(f'{i}년 사이트 주소 : {len(href_list)}개 수집')\n",
    "        print('-'*50)\n",
    "        print()\n",
    "\n",
    "    print(f'총 사이트 주소 개수 : {len(site_list)}개')\n",
    "    print('본문 수집 시작')\n",
    "\n",
    "    # 본문 수집\n",
    "    for site in tqdm(site_list):\n",
    "        get_content(site)\n",
    "\n",
    "    content_list.append(year_class)\n",
    "\n",
    "    # 컨텐츠 목록 json파일로 만들기\n",
    "    with open ('naver_blog.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_list, f, ensure_ascii=False, indent=4) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "사용할 때 변경할 부분은 이 아래가 전부입니다.\n",
    "year_list와 year_size만 설정하세요.\n",
    "\"\"\"\n",
    "# 수집할 연도 리스트(2020~2025년)\n",
    "year_list = [i for i in range(2020, 2026)]\n",
    "\n",
    "# 수집할 연도별 글 개수\n",
    "year_size = 1000\n",
    "\n",
    "# 통합 함수 실행\n",
    "get_json(year_list, year_size)\n",
    "\n",
    "\n",
    "driver.close() # 해당 브라우저만 닫기\n",
    "# driver.quit()  # 전체 브라우저 모두 닫기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
