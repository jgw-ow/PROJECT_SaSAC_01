{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from requests->webdriver-manager) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from requests->webdriver-manager) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\host\\anaconda3\\envs\\study\\lib\\site-packages (from requests->webdriver-manager) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "\n",
    "# 웹드라이버 설정\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "# 버전에 상관 없이 os에 설치된 크롬 브라우저 사용\n",
    "driver = webdriver.Chrome(service=webdriver.chrome.service.Service(ChromeDriverManager().install()), options=options)\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "# Naver API key 입력\n",
    "client_id = 'jOPryrZxBQqmttHYWA6k' \n",
    "client_secret = 'SmTcVxIiSD'\n",
    "\n",
    "# selenium으로 검색 페이지 불러오기 #\n",
    "naver_urls = []\n",
    "postdate = []\n",
    "titles = []\n",
    "\n",
    "# 검색어 입력\n",
    "keywords = input(\"검색할 키워드를 입력해주세요 (쉼표로 구분):\")\n",
    "keyword_list = keywords.split(',')\n",
    "encText = urllib.parse.quote(' '.join(keyword_list))\n",
    "\n",
    "# 검색을 끝낼 페이지 입력\n",
    "end = input(\"\\n크롤링을 끝낼 위치를 입력해주세요. (기본값:1, 최대값:100):\")  \n",
    "if end == \"\":\n",
    "    end = 1\n",
    "else:\n",
    "    end = int(end)\n",
    "print(\"\\n 1 ~ \", end, \"페이지 까지 크롤링을 진행 합니다\")\n",
    "\n",
    "# 한번에 가져올 페이지 입력\n",
    "display = input(\"\\n한번에 가져올 페이지 개수를 입력해주세요.(기본값:10, 최대값: 100):\")\n",
    "if display == \"\":\n",
    "    display = 10\n",
    "else:\n",
    "    display = int(display)\n",
    "print(\"\\n한번에 가져올 페이지 : \", display, \"페이지\")\n",
    "\n",
    "for start in range(1, end * display + 1, display):\n",
    "    url = f\"https://openapi.naver.com/v1/search/blog?query={encText}&start={start}&display={display}\"  # JSON 결과\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "    if rescode == 200:\n",
    "        response_body = response.read()\n",
    "        \n",
    "        data = json.loads(response_body.decode('utf-8'))['items']\n",
    "        for row in data:\n",
    "            if 'blog.naver' in row['link']:\n",
    "                naver_urls.append(row['link'])\n",
    "                postdate.append(row['postdate'])\n",
    "                title = row['title']\n",
    "                # html태그제거\n",
    "                pattern1 = '<[^>]*>'\n",
    "                title = re.sub(pattern=pattern1, repl='', string=title)\n",
    "                titles.append(title)\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        print(\"Error Code:\" + rescode)\n",
    "\n",
    "###naver 기사 본문 및 제목 가져오기###\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "contents = []\n",
    "comments_texts = []\n",
    "try:\n",
    "    for i in naver_urls:\n",
    "        print(i)\n",
    "        driver.get(i)\n",
    "        time.sleep(5)  # 대기시간 변경 가능\n",
    "\n",
    "        iframe = driver.find_element(By.ID , \"mainFrame\") # id가 mainFrame이라는 요소를 찾아내고 -> iframe임\n",
    "        driver.switch_to.frame(iframe) # 이 iframe이 내가 찾고자하는 html을 포함하고 있는 내용\n",
    "\n",
    "        source = driver.page_source\n",
    "        html = BeautifulSoup(source, \"html.parser\")\n",
    "        \n",
    "        # 기사 텍스트만 가져오기\n",
    "        content = html.select(\"div.se-main-container\")\n",
    "        #  list합치기\n",
    "        content = ''.join(str(content))\n",
    "\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = content.replace(pattern2, '')\n",
    "        content = content.replace('\\n', '')\n",
    "        content = content.replace('\\u200b', '')\n",
    "        contents.append(content)\n",
    "\n",
    "    news_df = pd.DataFrame({'title': titles, 'content': contents, 'date': postdate})\n",
    "    news_df.to_csv('blog.csv', index=False, encoding='utf-8-sig')\n",
    "except:\n",
    "    contents.append('error')\n",
    "    news_df = pd.DataFrame({'title': titles, 'content': contents, 'date': postdate})\n",
    "    news_df.to_csv('blog.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024년 12월 성수 팝업 - tvN 홀리데이 파티 (팝업 기간은...</td>\n",
       "      <td>[안녕하세여! 여러분~!  여러분~ 2025년 되었습니다!!  새해 복 많이 받으세...</td>\n",
       "      <td>20250101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[성수] 모푸샌드 팝업 스토어, 12월 마무리는 귀여운...</td>\n",
       "      <td>[ 예삐 학교가 12월 26, 27일을 재량 휴업일로 지정하는 바람에 휴가를 쓸 수...</td>\n",
       "      <td>20241227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12월 2주차 주간일기4 - 모푸샌드 mofusand 성수 팝업...</td>\n",
       "      <td>[불과 3년 전까지만 해도 우리 집은한 달에 한두 번은 쇼핑을 하러이마트나 코스트코...</td>\n",
       "      <td>20250102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유니클로 팝업 성수 히트텍 요원 되기?</td>\n",
       "      <td>[한때는 불매운동도 하더만 점점 이미지 좋아져서(?)성수 팝업도 오픈했어요하긴 겨울...</td>\n",
       "      <td>20241116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12월 성수 팝업 스토어 르쿠르제 팝업 예약없이 방문후기 (ft....</td>\n",
       "      <td>[    12월 성수 팝업스토어 일정 중 제일 핫한 르쿠르제 팝업  &amp;lt;Unfo...</td>\n",
       "      <td>20241206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>성수 팝업 핫플 추천, 빼빼로 미니 팝업 이벤트 참여해요!</td>\n",
       "      <td>[ 다양한 팝업스토어 구경하는 재미가 잇는성수동에서 이번에 빼빼로 팝업스토어가 오픈...</td>\n",
       "      <td>20241020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>뉴발란스 성수 993 팝업스토어 오픈, 아트갤러리 방문기</td>\n",
       "      <td>[본 포스팅은 ‘뉴발란스’ 로부터 소정의 원고료를 제공받아 솔직하게 작성된 후기입니...</td>\n",
       "      <td>20241017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>퍼셀 긱 사이언티스트 랩 성수 팝업(1등 당첨 방문후기)</td>\n",
       "      <td>[퍼셀을 알게 된건 신세계면세점!퍼셀 XXL 메이크오버 스틱 구매해서 써봤었는데코옆...</td>\n",
       "      <td>20241024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>11월 성수 팝업, 핫플 돌다가... 패딩 더현대서울팝업 정보 겟</td>\n",
       "      <td>[본 포스팅은 듀베티카로부터소정의 원고료를 제공받아 작성되었습니다 요즘 특히 핫한 ...</td>\n",
       "      <td>20241110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>10월 성수 팝업 본품 주는 루나 베이스 챔피언십 팝업스토어</td>\n",
       "      <td>[누구나 본품 받을 수 있는 10월 성수 팝업 루나 베이스 챔피언십 팝업스토어  안...</td>\n",
       "      <td>20241024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "0    2024년 12월 성수 팝업 - tvN 홀리데이 파티 (팝업 기간은...    \n",
       "1           [성수] 모푸샌드 팝업 스토어, 12월 마무리는 귀여운...    \n",
       "2      12월 2주차 주간일기4 - 모푸샌드 mofusand 성수 팝업...    \n",
       "3                        유니클로 팝업 성수 히트텍 요원 되기?   \n",
       "4     12월 성수 팝업 스토어 르쿠르제 팝업 예약없이 방문후기 (ft....    \n",
       "..                                         ...   \n",
       "495           성수 팝업 핫플 추천, 빼빼로 미니 팝업 이벤트 참여해요!   \n",
       "496            뉴발란스 성수 993 팝업스토어 오픈, 아트갤러리 방문기   \n",
       "497            퍼셀 긱 사이언티스트 랩 성수 팝업(1등 당첨 방문후기)   \n",
       "498       11월 성수 팝업, 핫플 돌다가... 패딩 더현대서울팝업 정보 겟   \n",
       "499          10월 성수 팝업 본품 주는 루나 베이스 챔피언십 팝업스토어   \n",
       "\n",
       "                                               content      date  \n",
       "0    [안녕하세여! 여러분~!  여러분~ 2025년 되었습니다!!  새해 복 많이 받으세...  20250101  \n",
       "1    [ 예삐 학교가 12월 26, 27일을 재량 휴업일로 지정하는 바람에 휴가를 쓸 수...  20241227  \n",
       "2    [불과 3년 전까지만 해도 우리 집은한 달에 한두 번은 쇼핑을 하러이마트나 코스트코...  20250102  \n",
       "3    [한때는 불매운동도 하더만 점점 이미지 좋아져서(?)성수 팝업도 오픈했어요하긴 겨울...  20241116  \n",
       "4    [    12월 성수 팝업스토어 일정 중 제일 핫한 르쿠르제 팝업  &lt;Unfo...  20241206  \n",
       "..                                                 ...       ...  \n",
       "495  [ 다양한 팝업스토어 구경하는 재미가 잇는성수동에서 이번에 빼빼로 팝업스토어가 오픈...  20241020  \n",
       "496  [본 포스팅은 ‘뉴발란스’ 로부터 소정의 원고료를 제공받아 솔직하게 작성된 후기입니...  20241017  \n",
       "497  [퍼셀을 알게 된건 신세계면세점!퍼셀 XXL 메이크오버 스틱 구매해서 써봤었는데코옆...  20241024  \n",
       "498  [본 포스팅은 듀베티카로부터소정의 원고료를 제공받아 작성되었습니다 요즘 특히 핫한 ...  20241110  \n",
       "499  [누구나 본품 받을 수 있는 10월 성수 팝업 루나 베이스 챔피언십 팝업스토어  안...  20241024  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('blog500.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022년 이후 데이터만 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# 웹드라이버 설정\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "# 버전에 상관 없이 os에 설치된 크롬 브라우저 사용\n",
    "driver = webdriver.Chrome(service=webdriver.chrome.service.Service(ChromeDriverManager().install()), options=options)\n",
    "driver.implicitly_wait(3)\n",
    "# Naver API key 입력\n",
    "client_id = 'jOPryrZxBQqmttHYWA6k'\n",
    "client_secret = 'SmTcVxIiSD'\n",
    "# selenium으로 검색 페이지 불러오기 #\n",
    "naver_urls = []\n",
    "postdate = []\n",
    "titles = []\n",
    "# 검색어 입력\n",
    "keywords = input(\"검색할 키워드를 입력해주세요 (쉼표로 구분):\")\n",
    "keyword_list = keywords.split(',')\n",
    "encText = urllib.parse.quote(' '.join(keyword_list))\n",
    "# 검색을 끝낼 페이지 입력\n",
    "end = input(\"\\n크롤링을 끝낼 위치를 입력해주세요. (기본값:1, 최대값:100):\")\n",
    "if end == \"\":\n",
    "    end = 1\n",
    "else:\n",
    "    end = int(end)\n",
    "print(\"\\n 1 ~ \", end, \"페이지 까지 크롤링을 진행 합니다\")\n",
    "# 한번에 가져올 페이지 입력\n",
    "display = input(\"\\n한번에 가져올 페이지 개수를 입력해주세요.(기본값:10, 최대값: 100):\")\n",
    "if display == \"\":\n",
    "    display = 10\n",
    "else:\n",
    "    display = int(display)\n",
    "print(\"\\n한번에 가져올 페이지 : \", display, \"페이지\")\n",
    "for start in range(1, end * display + 1, display):\n",
    "    url = f\"https://openapi.naver.com/v1/search/blog?query={encText}&start={start}&display={display}&sort=date\"  # JSON 결과, 최신순 정렬\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "    if rescode == 200:\n",
    "        response_body = response.read()\n",
    "        data = json.loads(response_body.decode('utf-8'))['items']\n",
    "        for row in data:\n",
    "            post_date = datetime.strptime(row['postdate'], '%Y%m%d')\n",
    "            if post_date.year >= 2022:\n",
    "                if 'blog.naver' in row['link']:\n",
    "                    naver_urls.append(row['link'])\n",
    "                    postdate.append(row['postdate'])\n",
    "                    title = row['title']\n",
    "                    # html태그제거\n",
    "                    pattern1 = '<[^>]*>'\n",
    "                    title = re.sub(pattern=pattern1, repl='', string=title)\n",
    "                    titles.append(title)\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        print(\"Error Code:\" + rescode)\n",
    "###naver 기사 본문 및 제목 가져오기###\n",
    "# ConnectionError방지\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "contents = []\n",
    "comments_texts = []\n",
    "try:\n",
    "    for i in naver_urls:\n",
    "        print(i)\n",
    "        driver.get(i)\n",
    "        time.sleep(5)  # 대기시간 변경 가능\n",
    "        iframe = driver.find_element(By.ID , \"mainFrame\") # id가 mainFrame이라는 요소를 찾아내고 -> iframe임\n",
    "        driver.switch_to.frame(iframe) # 이 iframe이 내가 찾고자하는 html을 포함하고 있는 내용\n",
    "        source = driver.page_source\n",
    "        html = BeautifulSoup(source, \"html.parser\")\n",
    "        # 기사 텍스트만 가져오기\n",
    "        content = html.select(\"div.se-main-container\")\n",
    "        #  list합치기\n",
    "        content = ''.join(str(content))\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = content.replace(pattern2, '')\n",
    "        content = content.replace('\\n', '')\n",
    "        content = content.replace('\\u200b', '')\n",
    "        contents.append(content)\n",
    "    news_df = pd.DataFrame({'title': titles, 'content': contents, 'date': postdate})\n",
    "    news_df.to_csv('blog.csv', index=False, encoding='utf-8-sig')\n",
    "except:\n",
    "    contents.append('error')\n",
    "    news_df = pd.DataFrame({'title': titles, 'content': contents, 'date': postdate})\n",
    "    news_df.to_csv('blog.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 ~ 1 페이지까지 크롤링을 진행합니다\n",
      "\n",
      "한번에 가져올 페이지: 10 페이지\n",
      "\n",
      "=== 검색 결과 ===\n",
      "검색 기간: 20220101 ~ 20221231\n",
      "총 게시물: 212481개\n",
      "현재 페이지: 1\n",
      "페이지 내 게시물: 10개\n",
      "==================\n",
      "\n",
      "현재까지 수집된 게시물: 10개\n",
      "URL 처리 중: https://blog.naver.com/hey_0622/223714533191\n",
      "URL 처리 중: https://blog.naver.com/alice7865/223714525742\n",
      "URL 처리 중: https://blog.naver.com/wogml2927/223714524844\n",
      "URL 처리 중: https://blog.naver.com/ivpb86/223714523559\n",
      "URL 처리 중: https://blog.naver.com/rastasew/223714514118\n",
      "URL 처리 중: https://blog.naver.com/eh_good/223714513034\n",
      "URL 처리 중: https://blog.naver.com/baewall/223714511852\n",
      "URL 처리 중: https://blog.naver.com/kidi01/223713412398\n",
      "URL 처리 중: https://blog.naver.com/10closet/223714504412\n",
      "URL 처리 중: https://blog.naver.com/wltn5818/223714503040\n",
      "크롤링 완료!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import urllib.request\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 웹드라이버 설정\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "driver = webdriver.Chrome(service=webdriver.chrome.service.Service(ChromeDriverManager().install()), options=options)\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "# Naver API key\n",
    "client_id = 'jOPryrZxBQqmttHYWA6k' \n",
    "client_secret = 'SmTcVxIiSD'\n",
    "\n",
    "# 데이터 저장용 리스트\n",
    "naver_urls = []\n",
    "postdate = []\n",
    "titles = []\n",
    "contents = []\n",
    "\n",
    "# 사용자 입력\n",
    "keywords = input(\"검색할 키워드를 입력해주세요 (쉼표로 구분):\")\n",
    "keyword_list = keywords.split(',')\n",
    "encText = urllib.parse.quote(' '.join(keyword_list))\n",
    "\n",
    "end = input(\"\\n크롤링을 끝낼 위치를 입력해주세요 (기본값:1, 최대값:100):\")\n",
    "end = 1 if end == \"\" else int(end)\n",
    "print(f\"\\n1 ~ {end} 페이지까지 크롤링을 진행합니다\")\n",
    "\n",
    "display = input(\"\\n한번에 가져올 페이지 개수를 입력해주세요 (기본값:10, 최대값:100):\")\n",
    "display = 10 if display == \"\" else int(display)\n",
    "print(f\"\\n한번에 가져올 페이지: {display} 페이지\")\n",
    "\n",
    "# 검색 기간 설정 (사용자 입력)\n",
    "dfrom = input(\"검색 시작 날짜를 입력해주세요 (YYYYMMDD): \")\n",
    "dto = input(\"검색 종료 날짜를 입력해주세요 (YYYYMMDD): \")\n",
    "\n",
    "try:\n",
    "    # 블로그 검색 API 호출\n",
    "    for start in range(1, end * display + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/blog?query={encText}&start={start}&display={display}&sort=date&dfrom={dfrom}&dto={dto}\"\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "        \n",
    "        try:\n",
    "            response = urllib.request.urlopen(request)\n",
    "            if response.getcode() == 200:\n",
    "                data = json.loads(response.read().decode('utf-8'))\n",
    "                \n",
    "                print(f\"\\n=== 검색 결과 ===\")\n",
    "                print(f\"검색 기간: {dfrom} ~ {dto}\")\n",
    "                print(f\"총 게시물: {data.get('total', 0)}개\")\n",
    "                print(f\"현재 페이지: {start//display + 1}\")\n",
    "                print(f\"페이지 내 게시물: {len(data.get('items', []))}개\")\n",
    "                print(\"==================\\n\")\n",
    "                \n",
    "                for item in data.get('items', []):\n",
    "                    if 'blog.naver' in item['link']:\n",
    "                        naver_urls.append(item['link'])\n",
    "                        postdate.append(item['postdate'])\n",
    "                        titles.append(re.sub('<[^>]*>', '', item['title']))\n",
    "                        \n",
    "                print(f\"현재까지 수집된 게시물: {len(titles)}개\")\n",
    "                time.sleep(2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"API 요청 중 오류: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 블로그 본문 크롤링\n",
    "    for i in naver_urls:\n",
    "        try:\n",
    "            print(f\"URL 처리 중: {i}\")\n",
    "            driver.get(i)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            iframe = driver.find_element(By.ID, \"mainFrame\")\n",
    "            driver.switch_to.frame(iframe)\n",
    "            \n",
    "            html = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            content = html.select(\"div.se-main-container\")\n",
    "            \n",
    "            # 날짜 정보 추출\n",
    "            date_element = html.select_one(\"span.se_publishDate\")\n",
    "            if date_element:\n",
    "                post_date = date_element.text.strip()\n",
    "            else:\n",
    "                post_date = 'date not found'\n",
    "            \n",
    "            if content:\n",
    "                content = re.sub('<[^>]*>', '', ''.join(str(content)))\n",
    "                content = content.replace('\\n', ' ').replace('\\u200b', '')\n",
    "            else:\n",
    "                content = 'content not found'\n",
    "                \n",
    "            contents.append(content)\n",
    "            postdate.append(post_date)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"컨텐츠 수집 중 오류: {e}\")\n",
    "            contents.append('error')\n",
    "            postdate.append('error')\n",
    "            \n",
    "    # 결과 저장\n",
    "    min_length = min(len(titles), len(contents), len(postdate))\n",
    "    news_df = pd.DataFrame({\n",
    "        'title': titles[:min_length],\n",
    "        'content': contents[:min_length],\n",
    "        'date': postdate[:min_length]\n",
    "    })\n",
    "    news_df.to_csv('blog_period.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"크롤링 완료!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"전체 처리 중 오류 발생: {e}\")\n",
    "finally:\n",
    "    driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
